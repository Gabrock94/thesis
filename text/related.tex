Approximate computing research combines insights from hardware engineering,
architecture, system design, programming languages, and even application
domains like machine learning.
This chapter summarizes research on implementing, exploiting,
controlling, and reasoning about approximation in computer systems.
To confine the scope, the survey focuses on work that exposes error to
applications (unlike fault tolerance, which seeks to \emph{hide}
errors),
and work that is in some sense general (not, for example, a new approximation
strategy for one specific graphics algorithm).

\section{Application Tolerance Studies}
\label{sec:related:studies}

Many authors have identified the property of error tolerance in existing
``soft''
applications. A large class of studies have examined this property by
injecting errors into certain parts of applications and assessing the
execution quality in terms of both crashes and output fidelity~\cite{li06,
li07, li08, dekruijf-selse09, wong-selse06, palem-arcs, freton, besteffort,
yeh, thaker-iiswc06, efc, llfi, chippa-dac}.
Related
studies have evaluated error-resilience in
integrated circuit designs~\cite{breuer, scalable-effort-hardware}.
This category of study repeatedly finds that
different parts of the
application have different impacts on reliability and fidelity.
Some conclude
that there is a useful distinction between critical and non-critical program
points, typically instructions~\cite{palem-arcs, thaker-iiswc06, flikker,
llfi}.
This conclusion reflects
the safety principle in \secref{princ:safety}:
certain program components, especially those involved in control flow, need to
be protected from all influence from approximation.

Much of the work on application robustness tends to assume an existing,
domain-specific notion of ``quality'' for each
application.
As the principle in \secref{princ:appspecific} suggests, these quality metrics
need careful consideration: one quality metric is not necessarily just as good
as another.
Recent work has proposed guidelines for measuring quality
rigorously~\cite{wddd-quality}.


\section{Exploiting Resilience in Architecture}

One category of techniques for approximation examines efficiency--accuracy
trade-offs in hardware.
These techniques can lead to gains in energy, performance, manufacturing
yield, or verification complexity.

\subsection{Functional Units}

Researchers have designed floating-point units
that dynamically adapt mantissa width~\cite{bitwidthred, hierarchfpu}, ``fuzzily'' memoize
similar arithmetic computations~\cite{fuzzymemo}, or tolerate timing
errors~\cite{kumarhpca, hizli, metafunctions}.
Alternative number
representations work in tandem with relaxed functional units to bound the
numerical error that can result from bit flips~\cite{stanleymarbell}.

The VLSI community has paid particular attention to variable-accuracy adder
designs, which are allowed to yield incorrect results for some minority of
input combinations~\cite{uva-adder, palem-adders, impact, adder-metrics,
configurable-adder, adder-iccad13, adder-tcad, adder-optimal, adder-dac12,
adder-isic09, adder-date08}.

\subsection{Memory}

SRAM
structures spend significant static power on retaining data, so they represent
another opportunity for fidelity trade-offs~\cite{hybrid-sram, sramerrors,
partially-forgetful}. Similarly,
DRAM structures can reduce the power spent on refresh cycles where bit flips
are allowed~\cite{flikker, sparkk}.
In persistent memories where storage cells can wear out, approximate systems
can reduce the number of bits they flip to lengthen the useful device
lifetime~\cite{fang-pcm}.
Similarly, low-power writes to memories like flash can exploit its
probabilistic properties while hiding them from software~\cite{halfwits,
powerfade, flash-retention-relax}.
Spintronic memories exhibit similarly favorable trade-offs between access cost
and error~\cite{spintronic-approx}.

These memory approximation techniques typically
work by exposing
soft errors and other analog effects.
Recent work in security and privacy has exploited these variability-related
errors to fingerprint hardware and deanonymize users~\cite{deanondram}.

\subsection{Circuit Design}

A broad category of work has proposed general techniques for making quality trade-offs
when synthesizing and optimizing general hardware
circuits~\cite{lossysynthesis, palem-pruning, rahimi, axilog, miao-thesis, synthesis-date14, venkataramani-date13, venkataramani-dac12}.
Other tools focus on analyzing approximate circuit
designs~\cite{venkatesan-iccad11, tziantzioulis-dac15}.

Near-threshold voltage domains also present a new opportunity for embracing
unpredictable circuit operation~\cite{soft-ntc}.

\subsection{Relaxed Fault Tolerance}

As a dual to adding errors in some circuits, some researchers have
explored differential fault protection in the face of universally unreliable
circuits. As process sizes continue to shrink, it is likely that reliable
transistors will become the minority; redundancy and checking will be
necessary to provide reliable operation~\cite{li-asplos08}. Circuit design
techniques have been proposed that reduce the cost of redundancy by providing
it selectively for certain instructions in a CPU~\cite{wreft} or certain
blocks in a DSP~\cite{unequal-protection, ant, micropower-dsp}.
Researchers at Wisconsin have applied relaxed error tolerance at the
architecture level to GPUs~\cite{palframan-gpu}.
Other work has used criticality information to selectively allocate
software-level error detection and correction
resources~\cite{khudia-tolerance, shi-cal}.

\subsection{Microarchitecture}

Aside from designing fundamentally approximate circuits, a different direction
introduces approximation by relaxing traditional microarchitectural
mechanisms.
Notably, ``soft coherence'' relaxes intercore
communication~\cite{softcoherence},
and load value approximation~\cite{lva-sanmiguel, lva-thwaites} approximates
numerical values instead of fetching them from main memory on cache misses.

Recent work has proposed system organizations that apply approximation at a
coarser grain.
Yetim et al., for instance,
describe a technique that uses coarse-grained quality techniques to
allow errors even in processor control logic~\cite{martonosi-date, commguard}.
Duwe~\cite{duwe-thesis} proposes run-time coalescing of approximate and
precise computations to reduce the overhead of switching between modes.
Other work allocates approximation among the lanes of a SIMD
unit~\cite{tabsh}.



\section{Exploiting Resilience with Program Transformations}
\label{sec:related:software}

Aside from hardware-level accuracy trade-offs, there are opportunities for
adapting \emph{algorithms} to execute with varying precision. Algorithmic
quality--complexity trade-offs are not new, but recent work has
proposed
tools for \emph{automatically} transforming programs to exhibit these
trade-offs.
Transformations include removing portions of a program's dynamic execution
(termed \emph{code perforation})~\cite{perforation}, unsound
parallelization of serial programs~\cite{quickstep}, eliminating
synchronization in parallel programs~\cite{dubstep, races-ibm, hogwild,
forgiving-parallel},
identifying and adjusting parameters that control output
quality~\cite{dynamicknobs}, randomizing portions of deterministic
programs~\cite{zhu-popl12, sasa-sas11}, dynamically choosing between
different programmer-provided implementations of the same
specification~\cite{green, virus, petabricks, taco-soc, ansel-autotuning,
scalable-classifier}, and replacing subcomputations with invocations
of a trained neural network~\cite{npu}.

Some work on algorithmic approximation targets specific hardware: notably,
general-purpose GPUs~\cite{paraprox, sage, herding, neuralgpu}.
In a GPU setting, approximation strategies benefit most by optimizing for
memory bandwidth and control divergence.

Recently, a research direction has developed in \emph{automated program
repair} and other approaches to heuristically patching software according to
programmer-specified criteria.
These techniques are typically approximate in that they abandon a traditional
compiler's goal of perfectly preserving the original program's semantics.
Notably, Schulte \etal~\cite{schulte} propose to use program evolution to
optimize for energy.

Precimonious~\cite{precimonious} addresses the problem of choosing appropriate
floating-point widths, which amount to a trade-off between numerical accuracy
and space or operation cost.
Similarly, STOKE's floating-point extension~\cite{stoke-fp} synthesizes new
versions of floating-point functions from scratch to meet different accuracy
requirements with optimal efficiency.

Neural acceleration is a recent technique treading code as a black box and
transforming it to a neural-network representation~\cite{npu, emeuro, benchnn,
temam-isca}.
It is, at its core, an algorithmic transformation, but it integrates tightly
with hardware support: a digital accelerator~\cite{npu}, analog
circuits~\cite{anpu}, FPGAs~\cite{snnap},
GPUs~\cite{neuralgpu}, or, recently, new analog substrates using
resistive memory~\cite{rram-npu} or memristors~\cite{memristor-npu}.
See \secref{npu} for a more detailed overview of neural acceleration.


\section{Exploiting Resilience in Other Systems}

While architecture optimizations and program transformations dominate the
field of proposed exploitations of approximate software, some recent work has
explored the same trade-off in other components of computer systems. Network
communication, with its reliance on imperfect underlying channels, exhibits
opportunities for fidelity trade-offs~\cite{softcast, luo-globecom, apex,
smpmup2006}. Notably, SoftCast~\cite{softcast} transmits images and video by
making the signal magnitude directly proportional to pixel luminance. BlinkDB,
a recent instance research on \emph{approximate query answering},
is a database system that can respond to queries that include a required
accuracy band on their output~\cite{blinkdb}.
Uncertain{\textless}T{\textgreater}~\cite{uncertaint} and Lax~\cite{lax}
propose to expose the approximate, probabilistic behavior of sensors at the
language and API levels.
By eschewing redundancy and
recovery in a fault-tolerant distributed system, researchers at Wisconsin were
able to make quality trade-offs in a supercomputing
setting~\cite{dekruijf-icpp}.


\section{Languages for Expressing Approximation}

Recently, language constructs that express and constrain
approximation have become a focus in the programming-languages research
community.
Relax~\cite{relax} is a language with ISA support for tolerating architectural
faults in software.
Rely~\cite{rely} uses specifications that relate the reliability of the input
to an approximate region of code to its outputs.

A related set of recent approximate-programming tools attempt to \emph{adapt}
a program to meet accuracy demands while using as few resources as possible.
Chisel~\cite{chisel} is an extension to Rely that searches for the subset of
operations in a program that can safely be made approximate.
ExpAX~\cite{expax-tr} finds safe-to-approximate operations automatically and
uses a metaheuristic to find which subset of them to actually approximate.

Some other programming systems that focus on energy efficiency include
approximation ideas:
Eon~\cite{eon} is a language for long-running embedded systems that can drop
tasks when energy resources are low,
and the Energy Types language~\cite{energytypes} incorporates a variety of
strategies for expressing energy requirements.


\section{Programmer Tools}

\subsection{Exploration and Debugging}

\cite{approxdebug}

\cite{qosprof}

\subsection{Algorithm Analysis}

Some verification tools and proof systems help
the programmer prove relationships between the original program and a
candidate relaxed version~\cite{carbin-pldi, carbin-races, carbin-pepm,
rice-transformation-semantics}.
Another approach constrains the programming model to help express programs
that refine their results as they run longer, permitting quality trade-offs in
hard real-time settings~\cite{chung90}.

\subsection{Dynamic Control}

As an alternative to statically bounding errors, dynamic techniques can
monitor quality degradation at run time.
The critical challenge for these techniques is balancing detection accuracy
with the added cost, which takes away from the efficiency advantages of
approximation.
Some work has suggested that programmers can provide domain-specific checks on
output quality~\cite{lwc, approxdebug}.
Recent work has explored automatic generation of error detectors~\cite{rumba}.
A variety of techniques propose mechanisms for run-time or profiling feedback to adapt
approximation parameters~\cite{dynamicknobs, green, approxit, ansel-autotuning}.
\TODO{describe these in a little more detail}


\section{Probabilistic Languages}

\TODO{way too much in this section}

Several research projects have proposed radically different models of
computation that, rather than adapting existing software to expose its error
resilience, are based exclusively on probabilistic reasoning. Most
prominently, Probabilistic CMOS~\cite{pcmos, pcmos-cacm, palem-dac-position}
and stochastic processors~\cite{stochasticproc, storm} expose the probabilistic
behavior of transistors as part of their ISA.
\TODO{More detail on stochastic computing.}
Other preliminary efforts from MIT~\cite{batesmit, lyric, mansinghka-circuits} and
Stanford~\cite{ersa} fall into the same category.

Researchers have proposed several languages and tools to
help developers better reason about and describe real-world
probabilistic data, computation, and models~\cite{BBGR13,
  wingate-lightweight, church, chaganty, pfeffersample, pmonad,
  infernet, probdsl,uncertaint}.
The probability monad captures a variable's discrete probability
distribution in functional programs~\cite{pmonad}.
\TODO{this one specific call-out is awkward}

Sankaranarayanan et al.~\cite{sriram-pldi} check assertions in
programs that produce probabilistic models using symbolic execution
and polyhedral volume estimation. The \texttt{estimateProbability} construct
queries the probability of an outcome, which resembles a probabilistic
assertion's
specification of the correct outcome.

Statistical model checking bounds verification error on
problems where state-space explosion makes exact numerical
verification intractable (see Legay and Delahaye's
survey~\cite{legay10}).  Model checking~\cite{Clarke} provides formal guarantees,
usually expressed in temporal logic for finite state
based models, often of hardware. For example, the
PRISM tool performs statistical verification of real-time
systems~\cite{KNP11}. Our work borrows the idea of hypothesis
testing to bound error in verification~\cite{Younes,Younes20061368}
and
relies on efficient sampling to avoid
the need for exhaustive state space exploration.

Similarly, recent work has sought to express
uncertainties in the context of traditional, imperative programming
languages~\cite{uncertaint}. These programming models incorporate algorithmic
probabilities---such as those inherent in sensor readings or machine learning
model parameters---but do not yet address the computational probabilities of
techniques like PCMOS and stochastic processors.
\TODO{describe UncertainT specifically}

Kozen~\cite{kozen} recognizes the need for semantics for programs
that use randomness during execution.
That work provides two semantics for a simple probabilistic
language---one that models sampling and one that computes on probability
distributions directly---and proves them equivalent.
Similarly, we prove equivalence between sampling an original program and
sampling its extracted Bayesian network representation.
Kozen predates the coinage and popularization of Bayesian networks, so
the semantics in that work are very different from the graphical-model approach
presented here.
Our Bayesian-network representation enables statistical optimizations that
make probabilistic assertion verification efficient.
\TODO{this paragraph has dangling references to the passert paper}

\subsection{Probabilistic Programming Languages}

The field of \emph{probabilistic programming} seeks to enable efficient
construction and querying of statistical models~\cite{BBGR13, wingate-lightweight,
  church, chaganty, pfeffersample, probdsl, koller}.  Experts write
generative models as programs and then inference algorithms answer questions
about the model's parameters. The canonical probabilistic programming example
answers, ``given that the grass is wet, was it due to rain or the sprinkler?''


\section{Robustness Analysis}

As the studies in Section~\ref{sec:related:studies} repeatedly find, error
tolerance varies greatly in existing software, both within and between
programs.
Independent of approximate computing, programming-languages researchers have
sought to identify and enhance error resilience properties.

SJava analyzes programs to prove that
errors only temporarily disrupt the execution path of a program~\cite{sjava}.
Program smoothing~\cite{smoothing-cav, smoothing-pldi, smoothing-fse} and
``robustification''~\cite{robustification} both find continuous, mathematical
functions that resemble the input--output behavior of numerical programs.
Auto-tuning approaches can help empirically identify error-resilient
components~\cite{asac}.
Finally, Cong and Gururaj describe a technique for automatically
distinguishing between critical and non-critical instructions for the purpose
of selective fault tolerance~\cite{cong-iccad}.
