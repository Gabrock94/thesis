\section{Introduction}
\label{sec:intro}

Accuracy and reliability are fundamental tenets in computer system design.
Programmers can expect that the processor never exposes timing
errors, and networking stacks typically aim to provide reliable transports
even on unreliable physical media.
When errors do occasionally happen, we treat them as exceptional outliers, not
as part of the system abstraction.
Cosmic rays can silently flip bits in DRAM, for example,
but the machine will typically use error-correcting codes to
maintain the illusion for programmers that the memory is infinitely reliable.

But abstractions with perfect accuracy come at a cost.
Chips need to choose conservative clock rates to banish timing errors,
storage and communication channels incur error-correction overhead,
and parallelism requires expensive synchronization.

Meanwhile, many applications have intrinsic tolerance to inaccuracy.
Applications in domains like computer vision, media
processing, machine learning, and sensor data analysis already incorporate
imprecision into their design.
Large-scale data analytics focus on aggregate trends rather than the integrity
of individual data elements.
In domains such as computer vision and robotics, there are no perfect answers:
results can vary in their usefulness, and the output quality is always in
tension with the resources that the software needs to produce them.
All these applications are \emph{approximate
programs}: a range of possible values can be considered ``correct'' outputs
for a given input.

From the perspective of an approximate program, today's
systems are overprovisioned with accuracy. Since the program is resilient, it
does
not need every arithmetic operation to be precisely correct and every bit of
memory to be preserved at the same level of reliability.
\emph{Approximate computing} is a research agenda that seeks to better match
the accuracy in system abstractions with the needs of approximate
programs.


\paragraph{Disciplined approximation}

The central challenge in approximate computing is forging abstractions that
make imprecision \emph{controlled and predictable} without sacrificing its
efficiency benefits.
This goal of this dissertation is to design hardware and software around
approximation-aware abstractions that, together, make accuracy--efficiency
trade-offs attainable for programmers.
My work examines approximate abstractions
in the contexts of programming languages, computer architecture,
memory technologies, compilers, and software development
tools.

\section{Research Principles}

The work in this dissertation is organized around a collection of principles
for the design of disciplined approximate abstractions.
These themes represent the collective findings of the concrete research
projects described later.

\subsection{Result Quality is Application Specific}

Since approximate computing navigates trade-offs between efficiency and result
quality, it needs definitions of both sides of the balance.
While \emph{efficiency} can have universal definitions---the time to
completion, for example, or the number of joules consumed---output
\emph{quality} is more subtle.
A key tenet in this work is is that applications must define ``output
quality'' case by case:
the platform cannot define quality without information from the programmer.

Following this philosophy, the system designs in this dissertation assume that
each approximate program comes with a \emph{quality metric}, expressed as
executable code, that scores the program's output on a continuous scale from
0.0 to 1.0.
A quality metric is the approximate-computing analog to a traditional software
\emph{specification}, which typically makes a binary decision about whether an
implementation is correct or incorrect.
Just as ordinary verification and optimization tools start from a
specification, approximate-computing tools start with a quality metric.

\subsection{Safety vs.~Quality}

At first glance, a quality metric seems like sufficient information to specify
an application's constraints on approximation.
If the system can guarantee that a program's output will always have a quality
score above $q$, and the programmer decides that $q$ is good enough, what
could possibly go wrong?

In reality, it can be difficult or impossible for systems to prove arbitrary
quality bounds with perfect certainty.
Realistic tools can often only certify, for example, that any output's quality
score will be at least $q$ \emph{with high probability},
or that \emph{nearly every output} will exceed quality $q$ but rare edge cases
may do worse.
Even more fundamentally, it can be difficult for programmers to devise formal
quality metrics that capture every possible factor in their intuitive notion
of output quality.
Quality metrics can be simpler if their scope is narrowed to data where they
are most relevant: the pixels in an output image, for example, but not the
header data.

To that end, this dissertation embraces \emph{safety} as a separate concept
from quality.
A safety property, in the context of approximate computing, is a guarantee
that part of a program \emph{never} deviates from its precise counterpart---in
other words, that it matches the semantics of a traditional, non-approximate
system.
A quality property, in contrast, constrains the \emph{amount} that approximate
program components deviate from their precise counterparts.

In practice, we find a first-order distinction between \emph{no approximation at
all} and \emph{approximation of some nonzero degree} both simplifies reasoning
for programmers and makes tools more tractable.
My work has demonstrated that the two kinds of properties can be amenable to
very different techniques:
information flow tracking (\chref{enerj}) is appropriate for safety, for
example, but statistical hypothesis testing (\chref{passert}) is better for
quality.

\subsection{Programming with Probabilistic Reasoning}

Often, the most natural ways to reason about approximation and quality use
probabilistic tools.
Probabilistic reasoning lets us show show statements such as \emph{this output
will be high-quality with at least probability $P$} or \emph{an input randomly
selected from this distribution leads to a high-quality output with
probability $P'$}.
These probabilistic statements can simultaneously match the nondeterministic behavior
of approximate systems~\cite{truffle, npu, approxstorage}
and correspond to software quality criteria~\cite{decaf, passert}.

To support reasoning about quality, approximate
programming models need to incorporate abstractions for statistical behavior.
The DECAF type system, in \chref{decaf}, and
probabilistic assertions, in \chref{passert}, represent two
complementary approaches to reasoning about probabilistic quality
properties.

These approaches dovetail with the recent expansion of interest in
\emph{probabilistic programming languages}, which seek to augment
machine-learning techniques with language abstractions~\cite{church}.
Approximate programming systems can adapt lessons from this body of research.

\subsection{Granularity of Approximation}

The \emph{granularity} at which approximate computing applies is a
nonintuitive but essential factor in its success.
My and other researchers' work has explored approximation strategies at
granularities of both extremes:
fine-grained approximations that apply to individual instructions and
individual words of memory (e.g., Truffle~\cite{truffle});
and coarse-grained approximations that holistically transform entire
algorithms (e.g., neural acceleration~\cite{npu}).

An technique's granularity affects its
generality and its efficiency potential.
A fine-grained approximation can be very general:
an approximate multiplier unit, for example, can potentially apply to any
multiplication in a program.
But the efficiency gains are fundamentally limited to \emph{non-control} components,
since control errors can disrupt execution arbitrarily.
Even if an approximate multiplier unit can be very efficient,
the same technique can never
improve the efficiency of a branch, an address calculation,
or even the
scheduling of an approximate multiply instruction.
Approximations that work at a coarser granularity can address control costs,
so their potential gains are larger.
But these techniques tend to apply more narrowly:
techniques that pattern-match on algorithm structures~\cite{paraprox},
for example, place nuanced restrictions on the code they can transform.

The EnerJ language in \chref{enerj} was initially designed for
fine-grained hardware approximation techniques such as low-voltage functional
units.
While the granularity was good for programmability, it was bad for efficiency:
our detailed hardware design for fine-grained hardware
approximation~\cite{truffle} demonstrated limited benefit.
The ACCEPT compiler in \chref{accept} bridges the gap: its analysis
library and optimizations exploit the fine-grained annotations from EnerJ to
safely apply coarse-grained optimizations.


\section{Abstractions for Disciplined Approximation}

This dissertation supports the above research principles using a set of
concrete system designs.
The systems comprise programming-language constructs that express
applications' resilience to approximation along with system-level
techniques for exploiting that latent resilience to gain efficiency.
This section serves as an overview of the interlocking designs;
Parts~\ref{part:programming} and~\ref{part:systems} give the full details.

\subsection{Controlling Safety and Quality}

The first set of projects consists of language abstractions that give
programmers control over safety and quality in approximate programs.

\subsubsection{Information Flow Tracking for General Safety}

EnerJ, described in \chref{enerj}, is a type system for enforcing safety in
the presence of approximation.
The key insight in EnerJ is that approximate programs tend to consist of two
intermixed kinds of storage and computation:
critical control components
and non-critical data components.
The latter, which typically form the majority of the program's execution,
are good candidates for approximation,
while the former should be protected from error and carry traditional
semantics.

EnerJ lets programmers enforce a separation between critical and non-critical
components.
It uses a type system that
borrows from static data
flow systems for security~\cite{jif} to provide a static
noninterference guarantee for precise data.
EnerJ extends Java with two type qualifiers, \code{@Approx} and
\code{@Precise}, and uses a subtyping relationship to prevent
approximate-to-precise information flow.
Using EnerJ, programmers can rely
on a proof that data marked as precise remains untainted by the errors arising
from approximation.

A key design goal in EnerJ is its \emph{generality:}
the language aims to encapsulate a range of approximation strategies under a
single abstraction.
Its type system covers approximate storage via the types of variables and
fields;
approximate processor logic via overloading of arithmetic operators;
and even user-defined approximate algorithms using dynamic method dispatch
based on its approximating qualifiers.

EnerJ addresses safety, not quality:
a variable with the type \code{@Approx float} can be arbitrarily incorrect and
EnerJ does not seek to bound its incorrectness.
By leaving the complementary concern of controlling quality to separate
mechanisms, EnerJ keeps its type system simple.

\subsubsection{Extending EnerJ with Probability Types}

DECAF, in \chref{decaf}, extends EnerJ's type-based approach to safety with
quality guarantees.
The idea is to generalize the original \code{@Approx} type qualifier to a
parameterized qualifier \code{@Approx($p$)}, where $p$ dictates the
\emph{degree} of approximation.
Specifically, in DECAF, $p$ is the lower bound on the probability that a value
is \emph{correct:} that the value in an approximate execution equals its
counterpart in a completely precise execution of the same program.
DECAF defines sound type rules for introducing and propagating these
correctness probabilities.

DECAF's added sophistication over EnerJ's simple two-level system comes at a
cost in complexity:
a type system that requires probability annotations on every expression,
however, would quickly become infeasible for programmers.
To mitigate annotation overhead, DECAF adds type inference.
Sparse probability annotations on the inputs and outputs of coarse-grained
subcomputations are typically enough for DECAF's inference system to determine
the less-intuitive probabilities for intermediate values.
Crucially, DECAF places no constraints on where programmers can write explicit
annotations:
developers can write probabilities where they make the most sense and leave
the remaining details to the compiler.

DECAF addresses the limitations of a conservative quality analysis using
an optional dynamic-tracking mechanism.
The inference system also allows efficient code reuse by specializing
functions according to the accuracy constraints of their calling contexts.

\subsubsection{Probabilistic Assertions}

DECAF's approach to controlling quality achieves strong probabilistic
guarantees by constraining the range of possible approximation strategies:
it works only with techniques where errors appear at an operation granularity;
whey they occur randomly but rarely; and when the error probability is
independent of the input values.

A complementary project takes the opposite approach:
it allows arbitrary probabilistic behavior
at the cost of weaker guarantees.
\TODO{finish}

\TODO{ dedicated (non-type) language construct --
not just about approximation}

\subsection{Exploiting Resilience for Efficiency}

The second category of research is on the implementation of systems that
exploit programs' tolerance for approximation to improve efficiency.
This dissertation describes two projects: an architectural technique and an
end-to-end compiler toolchain.
A primary concern in both systems is exposing an abstraction that fits with
the safety and quality constraints introduced in the above language
abstractions.

\subsubsection{Approximate Storage for Solid-State Memory Technologies}

Recently, I worked on a project that exploits the unique properties of
solid-state memory technologies, such as flash and phase-change memory,
to provide approximate storage. Our techniques use two properties common to these memories:
near-term wear-out and an underlying analog, continuous substrate. First, since
failed cells are commonplace, we can store approximate data in the otherwise
unusable regions of memory that contain these cells. Second, while memories
necessarily expose a digital interface to the rest of the system, the
underlying material---chalcogenide, in PCM's case---is intrinsically analog
and probabilistic. Exposing a limited amount of the material's stochastic
behavior enables savings in performance and power.
We simulated systems that use approximate PCM both as main memory and as
persistent, disk-like storage and demonstrated that our techniques can improve
memory write performance and extend the useful life of a memory module.

These designs are distinct from my other work in their focus on storage as
opposed to computation. An important lesson from this project is the
opportunity for reasoning about and controlling the probability distribution
of nondeterministic approximation errors. In both techniques, the distribution
of bit errors is decidedly nonuniform: the locations of failed bits is already
tracked by the memory system and analog storage substrates exhibit error
according to a distribution arising from the physical process. Because we have
information about the pattern of errors that occur in the approximate
memories, we were able to more effectively mitigate the errors' impact on the
application.

Our implementation performs this mitigation in an mostly application-agnostic
way: by prioritizing higher bits over lower ones, for example. But exposing
distribution information at other levels of the system stack could yield
further benefits. Exposing these properties to the programmer, for instance,
contrasts with EnerJ's simplistic approach of completely obscuring error
information but could enable more approximation opportunities.

\subsubsection{ACCEPT: An Approximate Compiler}

I am currently working on a compiler-based approach to optimizing approximate
programs, called ACCEPT (for Approximate C Compiler for Energy and Performance
Trade-offs). The project's goal is to harness the wide variety of
algorithmic approximation techniques enumerated in
Section~\ref{sec:related:software} with a compilation workflow that is both
\emph{automatic} and \emph{safe}. Using a lightweight annotation system
resembling EnerJ's, the compiler constrains the space of optimizations avoid
transformations that observably affect precise data.

The central challenge of this work is bridging the gap between fine-grained,
code-centric annotations to coarse, code-centric program transformations. To
reuse the averaging example from Section~\ref{sec:prelim:enerj}, ACCEPT
permits transformation (i.e., perforation~\cite{perforation}) of the
\ilcode{for} loop even though the loop control itself is not marked as
approximate. It accomplishes this by analyzing the effects of each candidate
code transformation and proving that changes can only affect the
observed values of approximate variables. Although it can affect the values of
precise \emph{intermediate} variables, ACCEPT permits a modified version of
EnerJ's guarantee: any \emph{observation} of a precise variable is identical
to the equivalent observation in a pristine execution.

In this sense, ACCEPT is an attempt to combine the potential efficiency gains
of coarse-grained approximation strategies like NPUs with the fine-grained
programmer control of Truffle and EnerJ. Since ACCEPT's program relaxations,
unlike NPUs' learning-based approach, are based on the \emph{implementation}
of approximate computations, the performance and precision impacts of each
transformation can be explained in terms of code changes to the original
program. ACCEPT exploits this property to provide programmer feedback that
illuminates which code changes are successful and which are prevented by the
current annotations. Developers can use this information to guide the compiler
to a better solution. This feedback loop resembles techniques used in
parallelizing compilers (e.g., Cray's~\cite{canal}).

While ACCEPT's whole-program analysis enables its coarse-grained
transformations, it also complicates the task of identifying the best such
transformation. The compiler needs to consider many types of optimizations as
they apply to many different portions of the program, and each optimization
may compose subtly with the others. Exploring this search space can require
a large number of executions, which may make it unworkable for large programs
when a human is in the loop. This shortcoming illustrates an important
pattern: a large scope of possible optimizations implies a large search space.
Techniques that make this search space faster to explore---by reducing the
size or simplifying the evaluation of each point---can help in generating
better approximations.



\section{Other Work}

The work in this document is intimately connected to other research I
collaborated on while at the University of Washington.
While this dissertation does not fully describe these related projects, their
influence is evident in the trajectory of projects that do appear here.
For context, this section describes a handful of other projects on approximate
hardware and developer tools.

\subsection{An Approximate CPU and ISA}

Truffle is a processor architecture that implements EnerJ's semantics to save
energy~\cite{truffle}. It uses a secondary, subcritical voltage that allows timing errors in
a portion of the logic and retention errors in a portion of the SRAM.
We
designed an ISA extension for approximate architectures that can serve as a
target for EnerJ-like languages that provides approximation per instruction,
per register, and per cache line.
The architecture uses replicated functional units, one running at the nominal
voltage and one running at the subcritical voltage, and steers instructions to the
appropriate unit based on a flag in the instruction. Each row in the SRAMs for
the register file and data cache carries a precision flag that controls the
voltage sent to the row; instructions control the precision of each line
dynamically by setting this flag.

To evaluate the Truffle design,
we reused the simulation infrastructure from the EnerJ evaluation and paired
it with a extension of a standard architectural energy model~\cite{mcpat}.
Results ranged from a 5\% energy efficiency \emph{loss} to a 43\%
consumption reduction.

These results emphasize the efficiency limits of very fine-grained
approximation. Even in a maximally approximate program---in which every
arithmetic instruction and every byte of memory is marked as
approximate---much of Truffle's energy is spent on precise work. Fetching
code, scheduling instructions, indexing into SRAMs, computing addresses, and
tracking precision state all must be performed reliably to avoid violating
the semantics of the EnerJ-like ISA.
Modern processors spend as much energy on
control as they do on computation itself, so any technique that optimizes only
computation will quickly encounter Amdahl's law.

Together, EnerJ and Truffle illustrate an essential shortcoming of
fine-grained approximation: it conservatively avoids relaxing \emph{control}
in terms of both the program (e.g., pointers and control flow) and the
execution substrate (e.g., fetch and decode).

\TODO{cite \cite{hadi-thesis}}

\subsection{Neural Acceleration}

Neural Processing Units represent a hybrid compiler--accelerator approach to
the problem of approximation granularity~\cite{npu}. The technique relaxes a
pure-functional portion of a program---a pure C function, in our
implementation---by observing its input/output behavior in a set of test runs.
This empirical characterization of the function is used to train a function
approximator based on an artificial neural network. Since neural networks have
efficient hardware implementations, the transformed function can be much
faster and lower-power than the original code. We designed a hardware neural
network accelerators to unlock these potential gains in transformed programs.

The NPU technique distinguishes itself from EnerJ and Truffle by applying
approximation based on observed inputs and outputs rather than the structure
of the code itself. This approach removes many practical restrictions on the
granularity of approximation: any computation with well-defined inputs and
outputs is a candidate for NPU-based relaxation. In our evaluation, we were
able to transform code regions that contained complex control flow, memory
allocation, and pointer arithmetic---all of which EnerJ and Truffle would have
classified as precise. As a result, our simulations demonstrated a 3$\times$
average energy reduction---far better than our results for Truffle.

But the coarser granularity comes at a cost of programmer visibility and
control. Since the NPU technique treats the target code as a black box, the
programmer has no direct influence over the performance and accuracy of the
resulting neural network. Instead, the programmer has to try applying the
transformation to a variety of candidate functions and hope that some will
yield a profitable accuracy--efficiency trade-off.
This shortcoming demonstrates a tension in my work so far between
programmability and efficiency.

\TODO{also cite \cite{snnap} \cite{hadi-thesis}}

\subsection{Monitoring and Debugging Quality}

I recently worked on a tool that addresses the programmability challenge posed
by nondeterministic approximation. Since testing can never capture
the full set of possible dynamic executions, uncertainty necessarily remains
when deploying software that can, via probabilistic behavior, go arbitrarily
wrong. Worse, even executions on the identical inputs can result in
widely varying result quality. Our system constrains the possible impacts of
nondeterministic approximation by directly measuring them and raising a
runtime exception when quality is degraded beyond a threshold. The program can
respond by re-executing the approximate computation or increasing its
precision level.

The central challenge of this work is the design of monitoring strategies that
are cheap enough to leave enabled in production. When performance is no object,
as it is during development, it suffices to use high-overhead approaches like our
own earlier work on prototyping and autotuning~\cite{enercaml} and Misailovic
\etal's profiler~\cite{qosprof}.
While no single low-overhead approach
generalizes to all approximate computations, we identify three strategies that
vary in their overhead, the kinds of computations they can monitor, and the
strength of their guarantees.

Our approaches have low overhead partially because they apply to
coarse-grained regions of code. Instead of checking the effects of
approximation after every arithmetic operation or when reading from any
approximate variable, we require the programmer to specify quality in terms of
the output of an entire approximate computation. This granularity choice
results in infrequent checks and allows programmers to write quality
evaluation where it is most natural: only at variables that represent outputs, not intermediate
values.

One important takeaway from this work is the utility of dynamic enforcement
to constrain the effects of nondeterminism and unknown probabilities. While
static analysis is infeasible when the pattern of approximation is unknown,
dynamic techniques can directly observe and cope with nondeterministic events.

\TODO{cite \cite{approxdebug} \cite{ringenburg-thesis}}



\section{Organization}

The next chapter is a literature survey of work on efficiency--accuracy
trade-offs.
Historical context is particularly important to this dissertation because the
fundamental idea of exchanging accuracy for returns in efficiency is so old:
analog computers and floating-point numbers, for example, are prototypical
approximate-computing strategies.

Parts~\ref{part:programming} and~\ref{part:systems} form the core of the
dissertation.
They comprise five independent but interlocking research projects that
together build up abstractions for making approximate computing both tractable
and efficient.
%
Part~\ref{part:programming} describes three approaches to abstracting
approximation in programming languages:
EnerJ, a type system that uses type qualifiers to make approximation safe;
DECAF, an extension of EnerJ that adds probabilistic reasoning
about the likelihood that data is correct;
and probabilistic assertions, a strategy for efficiently verifying complex
probabilistic properties via sampling.
%
Part~\ref{part:systems} describes two system designs for implementing
efficiency--accuracy trade-offs:
a hardware architecture that exploits the nuances of resistive memory
technologies such as phase-change memory;
and an open-source compiler toolkit that provides the scaffolding to quickly
implement new approximation strategies while balancing programmability with
approximation's potential benefits.

Finally, \plurchref{conclusion} and~\ref{ch:future} look forward and
backward, respectively.
The retrospective chapter distills lessons from the work in this dissertation
about approximate computing and hardware--software co-design in general, and
the prospective chapter suggests next steps for bringing approximation into
the mainstream.

This dissertation also includes appendices that formalize the
programming-languages techniques in Part~\ref{part:programming} and prove
their associated theorems.


\section{Previously Published Material}

This dissertation comprises work published elsewhere in conference papers:

\begin{itemize}
\item \chref{enerj}:
\textit{EnerJ: Approximate Data Types for Safe and General Low-Power
Computation.}
Adrian Sampson, Werner Dietl, Emily Fortuna, Danushen Gnanapragasam, Luis Ceze, and Dan Grossman.
In Programming Language Design and Implementation (PLDI), 2011.
\cite{enerj}

\item \chref{decaf}:
\textit{Probability Type Inference for Flexible Approximate Programming.}
Brett Boston, Adrian Sampson, Dan Grossman, and Luis Ceze.
To appear in OOPSLA 2015.
\cite{decaf}

\item \chref{passert}:
\textit{Expressing and Verifying Probabilistic Assertions.}
Adrian Sampson, Pavel Panchekha, Todd Mytkowicz, Kathryn McKinley, Dan Grossman, and Luis Ceze.
In Programming Language Design and Implementation (PLDI), 2014.
\cite{passert}

\item \chref{approxstorage}:
\textit{Approximate Storage in Solid-State Memories.}
Adrian Sampson, Jacob Nelson, Karin Strauss, and Luis Ceze.
In the IEEE/ACM International Symposium on Microarchitecture (MICRO), 2013.
\cite{approxstorage}
\end{itemize}
