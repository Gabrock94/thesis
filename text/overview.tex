\section{Introduction}
\label{sec:intro}

Computer systems typically provide strong, specific guarantees for their behavior. A
programmer can reasonably expect that the processor never exposes timing
errors and never loses a bit in its cache or main memory. When errors do
occasionally happen, such as when cosmic rays flips bits in DRAM, they are
treated as rare outliers rather than part of typical operation.
But providing perfect accuracy at all levels of the system comes at a high
cost. Circuits must be clocked so that they effectively never violate timing;
due to process variability, DRAM must be refreshed frequently enough to
account for the worst cell among the billions on a chip.

Meanwhile, many common applications have intrinsic tolerance to inaccuracies
in computation. Applications in domains like computer vision, media
processing, machine learning, and sensor data analysis already incorporate
imprecision in their operation. These applications are \emph{approximate
programs}: a range of possible values can be considered ``correct'' outputs
for a given input. From the perspective of an approximate program, today's
systems are overprovisioned with accuracy. Since the program is resilient, it
does
not need every arithmetic operation to be precisely correct and every byte of memory to be preserved
exactly. \emph{Approximate computing} is a set of techniques that exploit this
tolerance to trade off accuracy for resource consumption like performance and
energy efficiency.

However, applying approximation without discipline would make the construction
of reliable software nearly impossible. For approximate computation to be
usable, it must be confined to the error-tolerant parts of the program.  It
must not, for example, lead to uncontrolled jumps or wild pointers. My
research focuses on techniques that collaborate with the programmer to balance
efficiency and correctness. This research area, which I call \emph{disciplined
approximate programming}, provides approximate behavior in careful ways that
enable the construction of reliable software despite their own unreliability.

My work so far in this area has explored disciplined approximate programming
in the context of programming languages, CPU architecture, hardware
acceleration, storage technologies, profile-driven compilation, and programmer
tools. I describe these projects in Section~\ref{sec:prelim} to characterize
their successes and shortcomings. Using the lessons from this preliminary work
as background, I propose two new research projects in
Section~\ref{sec:proposed}. First, I describe a technique for analyzing
programs with nondeterministic approximation to characterize the probability
that they will produce acceptable output. Second, I propose an exploration
into applying fine-grained, statically controlled approximation to the process
of hardware design.


\section{Disciplined Approximation in Hardware and Software}

I plan to demonstrate in my dissertation that \textit{familiar programming
models can incorporate approximate computing in a way that enables large
efficiency gains without introducing unworkable complexity into the
development process}. There are two broad components to this thesis: the
design of programming languages and tools that safely incorporate approximate
semantics; and the implementation of efficient, approximate systems that match
those semantics. My dissertation will focus on co-designing these two
components with the goal of gaining efficiency without sacrificing
programmability.



In this section, I enumerate my completed and in-progress work on
disciplined approximate programming. The purpose of this section is to set the stage
for my proposed work by distilling lessons from the successes and shortcomings
of my prior work.unrelated 
To focus on the work most relevant to my thesis topic, I omit some ongoing
research that I am partially involved in to varying degrees. These include
projects on analog neural networks, neural networks on FPGAs, and approximate
wireless networking.
I also omit my unrelated work on safe parallel programming and
performance analysis for mobile Web browsers.

My work concerns the incorporation of approximate computing
into traditional execution models, existing
languages, and legacy software.
(A different approach, resembling probabilistic
programming~\cite{church}, would eschew traditional semantics for a completely
new programming model.)
I see the three themes of \emph{programmability},
\emph{granularity}, and \emph{nondeterminism} as underlying my work in this
area.

\paragraph{Programmability and safety.}

Imprecise results pose a threat to the feasibility of writing correct
programs. Clearly, a programming model in which arbitrary, incorrect results
are possible for any operation is too difficult to program. The programming
language and its associated runtime systems need to provide tools that let
programmers describe software's latent resilience to approximation. At the
same time, these tools must not add
untenable
complexity to the practice of software development. In my work, these
tools are in the form of lightweight extensions to traditional programming
languages.

\paragraph{Granularity and composability.}

When approximate computing is introduced in the context of a traditional
programming model, there are several choices for the kinds of computations
that are optimized with approximation. A system can approximate a whole
program or algorithm at once, a discrete program component like a function,
loop, or block, or individual operations (e.g., instructions).
Approximating an entire algorithm 
offers a large window of potential relaxations,
requiring only that a transformed version of the algorithm produce outputs
acceptably similar to the original.
But such a large space of possible optimizations can make a search of the
quality--efficiency trade-off space intractable, and black-box transformations
risk being opaque to programmer understanding and control.
Conversely, introducing approximation at the granularity of individual
instructions allows direct programmer control over imprecision. But it can
obscure the ways that local error patterns compose to affect the whole
program.

\paragraph{Nondeterminism and probabilities.}

Many of the techniques available for exploiting approximate applications do so
nondeterministically. For example, a low-voltage functional unit that allows
timing errors will compute correctly for most invocations but will produce wrong
answers with low probability. But the distribution is rarely uniform: the
probability and pattern of errors in an overclocked functional unit depends on
its input and different output bits will have different probabilities of being
correct. When multiple probabilistic components are invoked repeatedly and
their results combined, the impact on the computation is even more obscure.
Understanding and controlling probabilistic system components is critical to
composing them to do useful work.

\subsection{EnerJ: Safely Programming Approximate Systems}
\label{sec:prelim:enerj}

EnerJ is a type system for expressing programs' resilience to approximate
execution~\cite{enerj}. In EnerJ, programmers distinguish between the precise
portion of a program---those that should carry traditional semantics---and
parts that can safely be relaxed. EnerJ's type system borrows from static data
flow systems for security (seminally, Jif~\cite{jif}) to provide a static
noninterference guarantee for precise data. Using EnerJ, programmers can rely
on a proof that data marked as precise remains untainted by the errors arising
from approximation.

For EnerJ's evaluation, we annotated a variety of Java
programs and simulated an array of hardware approximation techniques to assess
their energy benefits and error impact. To assess the quality impact of
approximate computing, we developed application-specific \emph{quality
metrics} for each benchmark. A quality metric quantifies the application's
output fidelity by comparing it to the output of a precise execution on a
scale from 0.0 (completely useless) to 1.0 (identical to the pristine output).
I reused this quality metric concept in all of my subsequent evaluations for
later research. Quality measurement is critical to expressing programmer
intent and user expectations; while I do not explore it, there is an
opportunity for future work on helping programmers design good quality
metrics.

EnerJ's focus is on programmability. Using a lightweight, conceptually simple
annotation set, EnerJ programs can express approximation opportunities in
arithmetic operations, data elements, and algorithm selection. Programmers can
take advantage of a collection of disparate hardware approximation techniques
without reasoning carefully about the semantics of each. And its static
guarantees help quarantine the approximate component's nondeterminism while
incurring no runtime overhead.

In retrospect, EnerJ's programming model has two important limitations. First,
by providing approximation at a very fine grain---individual instructions and
cache lines---EnerJ constrains the parts of a program that can be
considered approximate. For example, consider this function (adapted from the original
paper) that computes the average of a list of numbers:

\begin{lstlisting}
@Approx float[] nums;
...
@Approx float total = 0.0;
for (int i = 0; i < nums.length; ++i)
  total += nums[i];
return total / nums.length;
\end{lstlisting}

The programmer has marked the input data, \ilcode{nums}, and the intermediate
variable \ilcode{total}, as approximate.
The loop iteration variable, \ilcode{i}, is implicitly marked as precise. This
is compulsory under EnerJ's semantics: since approximate variables can take
arbitrary values, approximating \ilcode{i} could lead to out-of-bounds errors
and even nontermination. (This is why EnerJ forbids approximate values from
being used in control flow and array indexing.) But a good approximation
of this code does not need perfectly precise iteration over \ilcode{nums}: an
acceptable computation could sample nondeterministically from the array.
Here, EnerJ's fine-grained approach to approximation forces it to be
conservative.

Second, EnerJ does not expose any information about probabilities to the
programmer. We made this design choice intentionally to allow EnerJ's
semantics to generalize over a wide variety of implementation techniques. But
while we need an initial distinction between critical components and those
that can tolerate even a small amount of error, opportunity is lost when
different parts of a program can tolerate more error---or even a different
distribution of errors---than others. Programmers must pessimistically
assume a worst-case approximate execution and therefore treat components with
intermediate error resilience as precise.

\subsection{Approximate Storage in Flash and Phase-Change Memory}

Recently, I worked on a project that exploits the unique properties of
solid-state memory technologies, such as flash and phase-change memory,
to provide approximate storage. Our techniques use two properties common to these memories:
near-term wear-out and an underlying analog, continuous substrate. First, since
failed cells are commonplace, we can store approximate data in the otherwise
unusable regions of memory that contain these cells. Second, while memories
necessarily expose a digital interface to the rest of the system, the
underlying material---chalcogenide, in PCM's case---is intrinsically analog
and probabilistic. Exposing a limited amount of the material's stochastic
behavior enables savings in performance and power.
We simulated systems that use approximate PCM both as main memory and as
persistent, disk-like storage and demonstrated that our techniques can improve
memory write performance and extend the useful life of a memory module.

These designs are distinct from my other work in their focus on storage as
opposed to computation. An important lesson from this project is the
opportunity for reasoning about and controlling the probability distribution
of nondeterministic approximation errors. In both techniques, the distribution
of bit errors is decidedly nonuniform: the locations of failed bits is already
tracked by the memory system and analog storage substrates exhibit error
according to a distribution arising from the physical process. Because we have
information about the pattern of errors that occur in the approximate
memories, we were able to more effectively mitigate the errors' impact on the
application.

Our implementation performs this mitigation in an mostly application-agnostic
way: by prioritizing higher bits over lower ones, for example. But exposing
distribution information at other levels of the system stack could yield
further benefits. Exposing these properties to the programmer, for instance,
contrasts with EnerJ's simplistic approach of completely obscuring error
information but could enable more approximation opportunities.

\subsection{ACCEPT: An Approximate Compiler}
\label{sec:prelim:accept}

I am currently working on a compiler-based approach to optimizing approximate
programs, called ACCEPT (for Approximate C Compiler for Energy and Performance
Trade-offs). The project's goal is to harness the wide variety of
algorithmic approximation techniques enumerated in
Section~\ref{sec:related:software} with a compilation workflow that is both
\emph{automatic} and \emph{safe}. Using a lightweight annotation system
resembling EnerJ's, the compiler constrains the space of optimizations avoid
transformations that observably affect precise data.

The central challenge of this work is bridging the gap between fine-grained,
code-centric annotations to coarse, code-centric program transformations. To
reuse the averaging example from Section~\ref{sec:prelim:enerj}, ACCEPT
permits transformation (i.e., perforation~\cite{perforation}) of the
\ilcode{for} loop even though the loop control itself is not marked as
approximate. It accomplishes this by analyzing the effects of each candidate
code transformation and proving that changes can only affect the
observed values of approximate variables. Although it can affect the values of
precise \emph{intermediate} variables, ACCEPT permits a modified version of
EnerJ's guarantee: any \emph{observation} of a precise variable is identical
to the equivalent observation in a pristine execution.

In this sense, ACCEPT is an attempt to combine the potential efficiency gains
of coarse-grained approximation strategies like NPUs with the fine-grained
programmer control of Truffle and EnerJ. Since ACCEPT's program relaxations,
unlike NPUs' learning-based approach, are based on the \emph{implementation}
of approximate computations, the performance and precision impacts of each
transformation can be explained in terms of code changes to the original
program. ACCEPT exploits this property to provide programmer feedback that
illuminates which code changes are successful and which are prevented by the
current annotations. Developers can use this information to guide the compiler
to a better solution. This feedback loop resembles techniques used in
parallelizing compilers (e.g., Cray's~\cite{canal}).

While ACCEPT's whole-program analysis enables its coarse-grained
transformations, it also complicates the task of identifying the best such
transformation. The compiler needs to consider many types of optimizations as
they apply to many different portions of the program, and each optimization
may compose subtly with the others. Exploring this search space can require
a large number of executions, which may make it unworkable for large programs
when a human is in the loop. This shortcoming illustrates an important
pattern: a large scope of possible optimizations implies a large search space.
Techniques that make this search space faster to explore---by reducing the
size or simplifying the evaluation of each point---can help in generating
better approximations.


\section{Organization}

The next chapter is a literature survey of work on efficiency--accuracy
trade-offs.
Historical context is particularly important to this dissertation because the
fundamental idea of exchanging accuracy for returns in efficiency is so old:
analog computers and floating-point numbers, for example, are prototypical
approximate-computing strategies.

Parts~\ref{part:programming} and~\ref{part:systems} form the core of the
dissertation.
They comprise five independent but interlocking research projects that
together build up abstractions for making approximate computing both tractable
and efficient.
%
Part~\ref{part:programming} describes three approaches to abstracting
approximation in programming languages:
EnerJ, a type system that uses type qualifiers to make approximation safe;
DECAF, an extension of EnerJ's type system that adds probabilistic reasoning
about the likelihood that data is correct;
and probabilistic assertions, a strategy for efficiently verifying complex
probabilistic properties via sampling.
%
Part~\ref{part:systems} describes two system designs for implementing
efficiency--accuracy trade-offs:
a hardware architecture that exploits the nuances of resistive memory
technologies such as phase-change memory;
and an open-source compiler toolkit that provides the scaffolding to quickly
implement new approximation strategies while balancing programmability with
approximation's potential benefits.

Finally, Chapters~\ref{ch:conclusion} and~\ref{ch:future} look forward and
backward, respectively.
The retrospective chapter distills lessons from the work in this dissertation
about approximate computing and hardware--software co-design in general, and
the prospective chapter suggests next steps for bringing approximation into
the mainstream.

This dissertation also includes appendices that formalize the
programming-languages techniques in Part~\ref{part:programming} and prove
their associated theorems.


\section{Previously Published Material}

This dissertation comprises work published elsewhere in conference papers:

\begin{itemize}
\item Chapter~\ref{ch:enerj}:
\textit{EnerJ: Approximate Data Types for Safe and General Low-Power
Computation.}
Adrian Sampson, Werner Dietl, Emily Fortuna, Danushen Gnanapragasam, Luis Ceze, and Dan Grossman.
In Programming Language Design and Implementation (PLDI), 2011.
\cite{enerj}

\item Chapter~\ref{ch:decaf}:
\textit{Probability Type Inference for Flexible Approximate Programming.}
Brett Boston, Adrian Sampson, Dan Grossman, and Luis Ceze.
To appear in OOPSLA 2015.
\cite{decaf}

\item Chapter~\ref{ch:passert}:
\textit{Expressing and Verifying Probabilistic Assertions.}
Adrian Sampson, Pavel Panchekha, Todd Mytkowicz, Kathryn McKinley, Dan Grossman, and Luis Ceze.
In Programming Language Design and Implementation (PLDI), 2014.
\cite{passert}

\item Chapter~\ref{ch:approxstorage}:
\textit{Approximate Storage in Solid-State Memories.}
Adrian Sampson, Jacob Nelson, Karin Strauss, and Luis Ceze.
In the IEEE/ACM International Symposium on Microarchitecture (MICRO), 2013.
\cite{approxstorage}
\end{itemize}
