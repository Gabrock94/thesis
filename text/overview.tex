\section{Introduction}
\label{sec:intro}

Accuracy and reliability are fundamental tenets in computer system design.
Programmers can expect that the processor never exposes timing
errors, and networking stacks typically aim to provide reliable transports
even on unreliable physical media.
When errors do occasionally happen, we treat them as exceptional outliers, not
as part of the system abstraction.
Cosmic rays can silently flip bits in DRAM, for example,
but the machine will typically use error-correcting codes to
maintain the illusion for programmers that the memory is infinitely reliable.

But abstractions with perfect accuracy come at a cost.
Chips need to choose conservative clock rates to banish timing errors,
storage and communication channels incur error-correction overhead,
and parallelism requires expensive synchronization.

Meanwhile, many applications have intrinsic tolerance to inaccuracy.
Applications in domains like computer vision, media
processing, machine learning, and sensor data analysis already incorporate
imprecision into their design.
Large-scale data analytics focus on aggregate trends rather than the integrity
of individual data elements.
In domains such as computer vision and robotics, there are no perfect answers:
results can vary in their usefulness, and the output quality is always in
tension with the resources that the software needs to produce them.
All these applications are \emph{approximate
programs}: a range of possible values can be considered ``correct'' outputs
for a given input.

From the perspective of an approximate program, today's
systems are overprovisioned with accuracy. Since the program is resilient, it
does
not need every arithmetic operation to be precisely correct and every bit of
memory to be preserved at the same level of reliability.
\emph{Approximate computing} is a research agenda that seeks to better match
the accuracy in system abstractions with the needs of approximate
programs.


\paragraph{Disciplined approximation.}

The central challenge in approximate computing is forging abstractions that
make imprecision \emph{controlled and predictable} without sacrificing its
efficiency benefits.
This goal of this dissertation is to design hardware and software around
approximation-aware abstractions that, together, make accuracy--efficiency
trade-offs attainable for programmers.
My work examines approximate abstractions
in the contexts of programming languages, computer architecture,
memory technologies, compilers, and software development
tools.

\section{Research Principles}

The work in this dissertation is organized around a collection of principles
for the design of disciplined approximate abstractions.
These themes represent the collective findings of the concrete research
projects described later.

\subsection{Result Quality is Application Specific}

Since approximate computing navigates trade-offs between efficiency and result
quality, it needs definitions of both sides of the balance.
While \emph{efficiency} can have universal definitions---the time to
completion, for example, or the number of joules consumed---output
\emph{quality} is more subtle.
A key tenet in this work is is that applications must define ``output
quality'' case by case:
the platform cannot define quality without information from the programmer.

Following this philosophy, the system designs in this dissertation assume that
each approximate program comes with a \emph{quality metric}, expressed as
executable code, that scores the program's output on a continuous scale from
0.0 to 1.0.
A quality metric is the approximate-computing analog to a traditional software
\emph{specification}, which typically makes a binary decision about whether an
implementation is correct or incorrect.
Just as ordinary verification and optimization tools start from a
specification, approximate-computing tools start with a quality metric.

\subsection{Safety vs.~Quality}

At first glance, a quality metric seems like sufficient information to specify
an application's constraints on approximation.
If the system can guarantee that a program's output will always have a quality
score above $q$, and the programmer decides that $q$ is good enough, what
could possibly go wrong?

In reality, it can be difficult or impossible for systems to prove arbitrary
quality bounds with perfect certainty.
Realistic tools can often only certify, for example, that any output's quality
score will be at least $q$ \emph{with high probability},
or that \emph{nearly every output} will exceed quality $q$ but rare edge cases
may do worse.
Even more fundamentally, it can be difficult for programmers to devise formal
quality metrics that capture every possible factor in their intuitive notion
of output quality.
Quality metrics can be simpler if their scope is narrowed to data where they
are most relevant: the pixels in an output image, for example, but not the
header data.

To that end, this dissertation embraces \emph{safety} as a separate concept
from quality.
A safety property, in the context of approximate computing, is a guarantee
that part of a program \emph{never} deviates from its precise counterpart---in
other words, that it matches the semantics of a traditional, non-approximate
system.
A quality property, in contrast, constrains the \emph{amount} that approximate
program components deviate from their precise counterparts.

In practice, we find a first-order distinction between \emph{no approximation at
all} and \emph{approximation of some nonzero degree} both simplifies reasoning
for programmers and makes tools more tractable.
The EnerJ type system in Chapter~\ref{ch:enerj} embodies this distinction by
building an abstraction for safety properties.
The quality-focused mechanisms in Chapters~\ref{ch:decaf} and~\ref{ch:passert}
complement EnerJ's safety constraints.

\subsection{Programming with Probabilistic Reasoning}

To support effective reasoning about approximation and quality,
programming models need to incorporate abstractions for statistical behavior.
\TODO{Chapters~\ref{ch:decaf} and~\ref{ch:passert}}

Many of the techniques available for exploiting approximate applications do so
nondeterministically. For example, a low-voltage functional unit that allows
timing errors will compute correctly for most invocations but will produce wrong
answers with low probability. But the distribution is rarely uniform: the
probability and pattern of errors in an overclocked functional unit depends on
its input and different output bits will have different probabilities of being
correct. When multiple probabilistic components are invoked repeatedly and
their results combined, the impact on the computation is even more obscure.
Understanding and controlling probabilistic system components is critical to
composing them to do useful work.

\subsection{Granularity of Approximation}

When approximate computing is introduced in the context of a traditional
programming model, there are several choices for the kinds of computations
that are optimized with approximation. A system can approximate a whole
program or algorithm at once, a discrete program component like a function,
loop, or block, or individual operations (e.g., instructions).
Approximating an entire algorithm
offers a large window of potential relaxations,
requiring only that a transformed version of the algorithm produce outputs
acceptably similar to the original.
But such a large space of possible optimizations can make a search of the
quality--efficiency trade-off space intractable, and black-box transformations
risk being opaque to programmer understanding and control.
Conversely, introducing approximation at the granularity of individual
instructions allows direct programmer control over imprecision. But it can
obscure the ways that local error patterns compose to affect the whole
program.

\section{Abstractions for Disciplined Approximation}

\TODO{Finish the overview. What goes here?}

\subsection{EnerJ: Safely Programming Approximate Systems}
\label{sec:prelim:enerj}

EnerJ is a type system for expressing programs' resilience to approximate
execution~\cite{enerj}. In EnerJ, programmers distinguish between the precise
portion of a program---those that should carry traditional semantics---and
parts that can safely be relaxed. EnerJ's type system borrows from static data
flow systems for security (seminally, Jif~\cite{jif}) to provide a static
noninterference guarantee for precise data. Using EnerJ, programmers can rely
on a proof that data marked as precise remains untainted by the errors arising
from approximation.

For EnerJ's evaluation, we annotated a variety of Java
programs and simulated an array of hardware approximation techniques to assess
their energy benefits and error impact. To assess the quality impact of
approximate computing, we developed application-specific \emph{quality
metrics} for each benchmark. A quality metric quantifies the application's
output fidelity by comparing it to the output of a precise execution on a
scale from 0.0 (completely useless) to 1.0 (identical to the pristine output).
I reused this quality metric concept in all of my subsequent evaluations for
later research. Quality measurement is critical to expressing programmer
intent and user expectations; while I do not explore it, there is an
opportunity for future work on helping programmers design good quality
metrics.

EnerJ's focus is on programmability. Using a lightweight, conceptually simple
annotation set, EnerJ programs can express approximation opportunities in
arithmetic operations, data elements, and algorithm selection. Programmers can
take advantage of a collection of disparate hardware approximation techniques
without reasoning carefully about the semantics of each. And its static
guarantees help quarantine the approximate component's nondeterminism while
incurring no runtime overhead.

In retrospect, EnerJ's programming model has two important limitations. First,
by providing approximation at a very fine grain---individual instructions and
cache lines---EnerJ constrains the parts of a program that can be
considered approximate. For example, consider this function (adapted from the original
paper) that computes the average of a list of numbers:

\begin{lstlisting}
@Approx float[] nums;
...
@Approx float total = 0.0;
for (int i = 0; i < nums.length; ++i)
  total += nums[i];
return total / nums.length;
\end{lstlisting}

The programmer has marked the input data, \ilcode{nums}, and the intermediate
variable \ilcode{total}, as approximate.
The loop iteration variable, \ilcode{i}, is implicitly marked as precise. This
is compulsory under EnerJ's semantics: since approximate variables can take
arbitrary values, approximating \ilcode{i} could lead to out-of-bounds errors
and even nontermination. (This is why EnerJ forbids approximate values from
being used in control flow and array indexing.) But a good approximation
of this code does not need perfectly precise iteration over \ilcode{nums}: an
acceptable computation could sample nondeterministically from the array.
Here, EnerJ's fine-grained approach to approximation forces it to be
conservative.

Second, EnerJ does not expose any information about probabilities to the
programmer. We made this design choice intentionally to allow EnerJ's
semantics to generalize over a wide variety of implementation techniques. But
while we need an initial distinction between critical components and those
that can tolerate even a small amount of error, opportunity is lost when
different parts of a program can tolerate more error---or even a different
distribution of errors---than others. Programmers must pessimistically
assume a worst-case approximate execution and therefore treat components with
intermediate error resilience as precise.

\subsection{Approximate Storage in Flash and Phase-Change Memory}

Recently, I worked on a project that exploits the unique properties of
solid-state memory technologies, such as flash and phase-change memory,
to provide approximate storage. Our techniques use two properties common to these memories:
near-term wear-out and an underlying analog, continuous substrate. First, since
failed cells are commonplace, we can store approximate data in the otherwise
unusable regions of memory that contain these cells. Second, while memories
necessarily expose a digital interface to the rest of the system, the
underlying material---chalcogenide, in PCM's case---is intrinsically analog
and probabilistic. Exposing a limited amount of the material's stochastic
behavior enables savings in performance and power.
We simulated systems that use approximate PCM both as main memory and as
persistent, disk-like storage and demonstrated that our techniques can improve
memory write performance and extend the useful life of a memory module.

These designs are distinct from my other work in their focus on storage as
opposed to computation. An important lesson from this project is the
opportunity for reasoning about and controlling the probability distribution
of nondeterministic approximation errors. In both techniques, the distribution
of bit errors is decidedly nonuniform: the locations of failed bits is already
tracked by the memory system and analog storage substrates exhibit error
according to a distribution arising from the physical process. Because we have
information about the pattern of errors that occur in the approximate
memories, we were able to more effectively mitigate the errors' impact on the
application.

Our implementation performs this mitigation in an mostly application-agnostic
way: by prioritizing higher bits over lower ones, for example. But exposing
distribution information at other levels of the system stack could yield
further benefits. Exposing these properties to the programmer, for instance,
contrasts with EnerJ's simplistic approach of completely obscuring error
information but could enable more approximation opportunities.

\subsection{ACCEPT: An Approximate Compiler}
\label{sec:prelim:accept}

I am currently working on a compiler-based approach to optimizing approximate
programs, called ACCEPT (for Approximate C Compiler for Energy and Performance
Trade-offs). The project's goal is to harness the wide variety of
algorithmic approximation techniques enumerated in
Section~\ref{sec:related:software} with a compilation workflow that is both
\emph{automatic} and \emph{safe}. Using a lightweight annotation system
resembling EnerJ's, the compiler constrains the space of optimizations avoid
transformations that observably affect precise data.

The central challenge of this work is bridging the gap between fine-grained,
code-centric annotations to coarse, code-centric program transformations. To
reuse the averaging example from Section~\ref{sec:prelim:enerj}, ACCEPT
permits transformation (i.e., perforation~\cite{perforation}) of the
\ilcode{for} loop even though the loop control itself is not marked as
approximate. It accomplishes this by analyzing the effects of each candidate
code transformation and proving that changes can only affect the
observed values of approximate variables. Although it can affect the values of
precise \emph{intermediate} variables, ACCEPT permits a modified version of
EnerJ's guarantee: any \emph{observation} of a precise variable is identical
to the equivalent observation in a pristine execution.

In this sense, ACCEPT is an attempt to combine the potential efficiency gains
of coarse-grained approximation strategies like NPUs with the fine-grained
programmer control of Truffle and EnerJ. Since ACCEPT's program relaxations,
unlike NPUs' learning-based approach, are based on the \emph{implementation}
of approximate computations, the performance and precision impacts of each
transformation can be explained in terms of code changes to the original
program. ACCEPT exploits this property to provide programmer feedback that
illuminates which code changes are successful and which are prevented by the
current annotations. Developers can use this information to guide the compiler
to a better solution. This feedback loop resembles techniques used in
parallelizing compilers (e.g., Cray's~\cite{canal}).

While ACCEPT's whole-program analysis enables its coarse-grained
transformations, it also complicates the task of identifying the best such
transformation. The compiler needs to consider many types of optimizations as
they apply to many different portions of the program, and each optimization
may compose subtly with the others. Exploring this search space can require
a large number of executions, which may make it unworkable for large programs
when a human is in the loop. This shortcoming illustrates an important
pattern: a large scope of possible optimizations implies a large search space.
Techniques that make this search space faster to explore---by reducing the
size or simplifying the evaluation of each point---can help in generating
better approximations.



\section{Other Work}

The work in this document is intimately connected to other research I
collaborated on while at the University of Washington.
While this dissertation does not fully describe these related projects, their
influence is evident in the trajectory of projects that do appear here.
For context, this section describes a handful of other projects on approximate
hardware and developer tools.

\subsection{An Approximate CPU and ISA}

Truffle is a processor architecture that implements EnerJ's semantics to save
energy~\cite{truffle}. It uses a secondary, subcritical voltage that allows timing errors in
a portion of the logic and retention errors in a portion of the SRAM.
We
designed an ISA extension for approximate architectures that can serve as a
target for EnerJ-like languages that provides approximation per instruction,
per register, and per cache line.
The architecture uses replicated functional units, one running at the nominal
voltage and one running at the subcritical voltage, and steers instructions to the
appropriate unit based on a flag in the instruction. Each row in the SRAMs for
the register file and data cache carries a precision flag that controls the
voltage sent to the row; instructions control the precision of each line
dynamically by setting this flag.

To evaluate the Truffle design,
we reused the simulation infrastructure from the EnerJ evaluation and paired
it with a extension of a standard architectural energy model~\cite{mcpat}.
Results ranged from a 5\% energy efficiency \emph{loss} to a 43\%
consumption reduction.

These results emphasize the efficiency limits of very fine-grained
approximation. Even in a maximally approximate program---in which every
arithmetic instruction and every byte of memory is marked as
approximate---much of Truffle's energy is spent on precise work. Fetching
code, scheduling instructions, indexing into SRAMs, computing addresses, and
tracking precision state all must be performed reliably to avoid violating
the semantics of the EnerJ-like ISA.
Modern processors spend as much energy on
control as they do on computation itself, so any technique that optimizes only
computation will quickly encounter Amdahl's law.

Together, EnerJ and Truffle illustrate an essential shortcoming of
fine-grained approximation: it conservatively avoids relaxing \emph{control}
in terms of both the program (e.g., pointers and control flow) and the
execution substrate (e.g., fetch and decode).

\TODO{cite \cite{hadi-thesis}}

\subsection{Neural Acceleration}

Neural Processing Units represent a hybrid compiler--accelerator approach to
the problem of approximation granularity~\cite{npu}. The technique relaxes a
pure-functional portion of a program---a pure C function, in our
implementation---by observing its input/output behavior in a set of test runs.
This empirical characterization of the function is used to train a function
approximator based on an artificial neural network. Since neural networks have
efficient hardware implementations, the transformed function can be much
faster and lower-power than the original code. We designed a hardware neural
network accelerators to unlock these potential gains in transformed programs.

The NPU technique distinguishes itself from EnerJ and Truffle by applying
approximation based on observed inputs and outputs rather than the structure
of the code itself. This approach removes many practical restrictions on the
granularity of approximation: any computation with well-defined inputs and
outputs is a candidate for NPU-based relaxation. In our evaluation, we were
able to transform code regions that contained complex control flow, memory
allocation, and pointer arithmetic---all of which EnerJ and Truffle would have
classified as precise. As a result, our simulations demonstrated a 3$\times$
average energy reduction---far better than our results for Truffle.

But the coarser granularity comes at a cost of programmer visibility and
control. Since the NPU technique treats the target code as a black box, the
programmer has no direct influence over the performance and accuracy of the
resulting neural network. Instead, the programmer has to try applying the
transformation to a variety of candidate functions and hope that some will
yield a profitable accuracy--efficiency trade-off.
This shortcoming demonstrates a tension in my work so far between
programmability and efficiency.

\TODO{also cite \cite{snnap} \cite{hadi-thesis}}

\subsection{Monitoring and Debugging Quality}

I recently worked on a tool that addresses the programmability challenge posed
by nondeterministic approximation. Since testing can never capture
the full set of possible dynamic executions, uncertainty necessarily remains
when deploying software that can, via probabilistic behavior, go arbitrarily
wrong. Worse, even executions on the identical inputs can result in
widely varying result quality. Our system constrains the possible impacts of
nondeterministic approximation by directly measuring them and raising a
runtime exception when quality is degraded beyond a threshold. The program can
respond by re-executing the approximate computation or increasing its
precision level.

The central challenge of this work is the design of monitoring strategies that
are cheap enough to leave enabled in production. When performance is no object,
as it is during development, it suffices to use high-overhead approaches like our
own earlier work on prototyping and autotuning~\cite{enercaml} and Misailovic
\etal's profiler~\cite{qosprof}.
While no single low-overhead approach
generalizes to all approximate computations, we identify three strategies that
vary in their overhead, the kinds of computations they can monitor, and the
strength of their guarantees.

Our approaches have low overhead partially because they apply to
coarse-grained regions of code. Instead of checking the effects of
approximation after every arithmetic operation or when reading from any
approximate variable, we require the programmer to specify quality in terms of
the output of an entire approximate computation. This granularity choice
results in infrequent checks and allows programmers to write quality
evaluation where it is most natural: only at variables that represent outputs, not intermediate
values.

One important takeaway from this work is the utility of dynamic enforcement
to constrain the effects of nondeterminism and unknown probabilities. While
static analysis is infeasible when the pattern of approximation is unknown,
dynamic techniques can directly observe and cope with nondeterministic events.

\TODO{cite \cite{approxdebug} \cite{ringenburg-thesis}}



\section{Organization}

The next chapter is a literature survey of work on efficiency--accuracy
trade-offs.
Historical context is particularly important to this dissertation because the
fundamental idea of exchanging accuracy for returns in efficiency is so old:
analog computers and floating-point numbers, for example, are prototypical
approximate-computing strategies.

Parts~\ref{part:programming} and~\ref{part:systems} form the core of the
dissertation.
They comprise five independent but interlocking research projects that
together build up abstractions for making approximate computing both tractable
and efficient.
%
Part~\ref{part:programming} describes three approaches to abstracting
approximation in programming languages:
EnerJ, a type system that uses type qualifiers to make approximation safe;
DECAF, an extension of EnerJ that adds probabilistic reasoning
about the likelihood that data is correct;
and probabilistic assertions, a strategy for efficiently verifying complex
probabilistic properties via sampling.
%
Part~\ref{part:systems} describes two system designs for implementing
efficiency--accuracy trade-offs:
a hardware architecture that exploits the nuances of resistive memory
technologies such as phase-change memory;
and an open-source compiler toolkit that provides the scaffolding to quickly
implement new approximation strategies while balancing programmability with
approximation's potential benefits.

Finally, Chapters~\ref{ch:conclusion} and~\ref{ch:future} look forward and
backward, respectively.
The retrospective chapter distills lessons from the work in this dissertation
about approximate computing and hardware--software co-design in general, and
the prospective chapter suggests next steps for bringing approximation into
the mainstream.

This dissertation also includes appendices that formalize the
programming-languages techniques in Part~\ref{part:programming} and prove
their associated theorems.


\section{Previously Published Material}

This dissertation comprises work published elsewhere in conference papers:

\begin{itemize}
\item Chapter~\ref{ch:enerj}:
\textit{EnerJ: Approximate Data Types for Safe and General Low-Power
Computation.}
Adrian Sampson, Werner Dietl, Emily Fortuna, Danushen Gnanapragasam, Luis Ceze, and Dan Grossman.
In Programming Language Design and Implementation (PLDI), 2011.
\cite{enerj}

\item Chapter~\ref{ch:decaf}:
\textit{Probability Type Inference for Flexible Approximate Programming.}
Brett Boston, Adrian Sampson, Dan Grossman, and Luis Ceze.
To appear in OOPSLA 2015.
\cite{decaf}

\item Chapter~\ref{ch:passert}:
\textit{Expressing and Verifying Probabilistic Assertions.}
Adrian Sampson, Pavel Panchekha, Todd Mytkowicz, Kathryn McKinley, Dan Grossman, and Luis Ceze.
In Programming Language Design and Implementation (PLDI), 2014.
\cite{passert}

\item Chapter~\ref{ch:approxstorage}:
\textit{Approximate Storage in Solid-State Memories.}
Adrian Sampson, Jacob Nelson, Karin Strauss, and Luis Ceze.
In the IEEE/ACM International Symposium on Microarchitecture (MICRO), 2013.
\cite{approxstorage}
\end{itemize}
