% \xxx[Luis]{We should consider moving the related work to the back of the paper.}
% \xxx[Kathryn]{Hmm, we should make it shorter, but the point of this
%   section should be to show that we are so broad we can help all these
%   different areas.} \xxx[Luis]{That makes sense, but as it is, this section is a lot about what we are different or similar to, rather how we can help these related problems/areas. An option is to split this into Related work and Uses/applications, move the former to the end, leave the latter as the second section. But it is fine, we don't need to move it, since the new intro now offers a really nice overview of the whole approach our paper proposes.}


\section{Related Work}

Researchers have proposed several languages and tools to
help developers better reason about and describe real-world
probabilistic data, computation, and models~\cite{BBGR13,
  wingate-lightweight, church, chaganty, pfeffersample, pmonad,
  infernet, probdsl,uncertaint}.
This section compares prior efforts with our work.
At a high level, our approach lets programmers use traditional language
features (e.g.,~calls
to C's \code{rand()}) to express probabilistic semantics and a simple
construct to encode probabilistic correctness properties.
% Our analysis of random
% variables is distinct from statistical model
% checking~\cite{KNP11,Younes,Younes20061368}, which reasons probabilistically
% about discrete-valued programs variables and does not use statistical
% properties to make sampling efficient.

\paragraph{Semantics and Verification of Probabilistic Programs}
The probability monad captures a variable's discrete probability
distribution in functional programs~\cite{pmonad}.
% This approach
% demonstrates three queries---expectation, sampling, and support---but it
% does not incorporate hypothesis testing or define a semantics for
% conditionals. 
Similarly, \unct{T} uses the monadic technique of
building up a computation tree and then querying it and adds sampling and
hypothesis testing to evaluate conditionals~\cite{uncertaint}.
We build on this work by extending it to the problem of
verification, by applying symbolic execution to summarize many
program paths, and by adding the concept of optimization via
statistical properties.

Sankaranarayanan et al.~\cite{sriram-pldi} check assertions in
programs that produce probabilistic models using symbolic execution
and polyhedral volume estimation. The \code{estimateProbability} construct
queries the probability of an outcome, which resembles \passert's
specification of the correct outcome.
That work's polyhedral approach avoids sampling but limits the technique in
important ways: it works only with
distributions for which a cumulative distribution function is available
and programs that use only linear arithmetic over these distributions.
Our distribution extraction approach uses sampling to generalize to a broader
class of
probabilistic programs.

% Finally, 
Statistical model checking bounds verification error on 
problems where state-space explosion makes exact numerical
verification intractable (see Legay and Delahaye's
survey~\cite{legay10}).  Model checking~\cite{Clarke} provides formal guarantees,
usually expressed in temporal logic for finite state
based models, often of hardware. For example, the
PRISM tool performs statistical verification of real-time
systems~\cite{KNP11}. Our work borrows the idea of hypothesis
testing to bound error in verification~\cite{Younes,Younes20061368}
and
relies on efficient sampling to avoid
the need for exhaustive state space exploration.

Kozen~\cite{kozen} recognizes the need for semantics for programs
that use randomness during execution.
That work provides two semantics for a simple probabilistic
language---one that models sampling and one that computes on probability
distributions directly---and proves them equivalent.
Similarly, we prove equivalence between sampling an original program and
sampling its extracted Bayesian network representation.
Kozen predates the coinage and popularization of Bayesian networks, so
the semantics in that work are very different from the graphical-model approach
presented here.
Our Bayesian-network representation enables statistical optimizations that
make \passert verification efficient.

\paragraph{Probabilistic Programming}
The field of \emph{probabilistic programming} seeks to enable efficient
construction and querying of statistical models~\cite{BBGR13, wingate-lightweight,
  church, chaganty, pfeffersample, probdsl, koller}.  Experts write
generative models as programs and then inference algorithms answer questions
about the model's parameters. The canonical probabilistic programming example
answers, ``given that the grass is wet, was it due to rain or the sprinkler?''

% There are many practical applications for this model of probabilistic
% programs (some of which we explore below). Although there are many
% similarities, the probabilistic programs we consider are distinct from
% the kinds of programs explored in research on probabilistic
% programming, where the purpose is describing probabilistic models in
% order to perform inference over them~\cite{church, infernet, ibal}.

% These programming models infer latent parameters that give rise to
% observed evidence.  
In contrast, we focus on traditional computation
of outputs based on user-specified inputs and do not incorporate
\emph{conditioning} (as in Infer.NET's constraints~\cite{infernet} and in
Church's query evidence~\cite{church}). Our analysis instead supports
general, potentially unanalyzable code that produces arbitrary
probability distributions.  These differences mean that our techniques
apply to ``probabilistic programming languages'' in the more traditional sense as defined
by Kozen~\cite{kozen}: typical imperative languages that include random calls.


% \paragraph{Application Domains} Probabilistic assertions capture
% properties of programs that compute with probabilistic data and
% programs whose computations themselves are probabilistic.  These
% increasingly programs include mobile applications that read sensors,
% perform data obfuscation, and approximate programming.

% \paragraph{Computing on Uncertain Inputs}
% Programs that compute on noisy data, such as GPS,
% accelerometers, and robot sensor data, behave
% probabilistically~\cite{PPT:05,uncertaint}. % In these programs, the
% % software conceptually reads an exact value, perturbs it according to a
% % probability distribution that models the sensor's imperfection.
% Programmers must reason about how random errors in sensor readings
% impact program to compute correctly on this data. For example, a mobile application that
% uses sensor readings to infer the user's physical activities
% propagates errors from its inputs to its outputs.  The correctness of
% this program depends on the chance that a sensor error causes an
% incorrect activity determination.  Checking a probabilistic assertion
% on the output requires a model of the error distribution on the
% inputs---for example,  empirical measurements of accelerometer 
% and GPS sensor errors~\cite{VD:07,T:98}.

\paragraph{Data Obfuscation for Privacy}
Recent work has focused on proving that obfuscated queries effectively
obscure private data via \emph{differential privacy}~\cite{pinq,
airavat, gupt, fuzz, certipriv}. Probabilistic assertions, in contrast, do not check
privacy. They instead solve the complementary (and less well-studied) problem
of verifying the utility of private computations.

% \xxx[Kathryn]{The approximate programming  section needs to be a lot
%   shorter, but I am out of steam.}

% few errors leading to wrong pixels in an output image, for example, are
% unlikely to render an execution useless. 

% In this sense, an approximate program executing on unreliable hardware is
% a probabilistic program. that implicitly makes calls to probabilistic error
% injection functions.
% To understand the impact of approximation on a
% program's output quality.  


\paragraph{Approximate Computing}
Approximate computing techniques exploit the inherent resilience of many
applications to execute them more efficiently at the cost of occasional
errors~\cite{enerj, npu, rely, perforation}.
A central challenge in approximate computing is analyzing programs to
determine the impact of approximation on a computation's overall accuracy.
Previous efforts have used static analysis to prove statistical bounds on the
difference between an original program and a version that elides some
operations~\cite{sasa-sas,zhu-popl}.
Rather than analyzing probability distributions introduced by the program, this technique assumes
that inputs are selected randomly and analyzes specific program patterns that
involve them.
In contrast, Rely~\cite{rely} checks the probability that nondeterministic
operations with a binary chance of failure compound to corrupt a computation's
output.
Carbin et al.~\cite{carbin-pldi} provide a system for proving properties that
must hold even when errors occur.
Our technique improves on these prior approaches by reasoning about the effects of
full probability distributions on approximate programs.

% Whereas this prior work focused on
% efficiency gains, leaving correctness to programmers, adding and
% validating \passerts in approximate programs gives programmers a tool
% to express this quality, e.g., requiring that pixel errors are less
% than 5\% in an image renderer.  Many error distributions in
% approximate hardware and software lack closed forms and must be
% described with empirical distributions, which our approach handles.

% The approximate computing literature
% typically assumes that programmers provide output quality metrics that
% quantify the error for a given output~\cite{enerj, truffle, npu}.
% Using probabilistic assertions, programmers can specify that the
% output quality must be likely to exceed a threshold.  For example, the
% programmer might write an probabilistic assertion for an image
% renderer that constrains the probability that the total pixel
% luminance is off by at most 10\%.  Alternatively, one could write two
% different approximations of an algorithm and determine the probability
% that one's quality exceeds the other.

% The probability distributions involved in approximate computing can be
% complex and many lack closed forms.  For example, an approximate
% program that runs on hardware with approximate
% storage~\cite{approxstorage} might draw from a probability
% distribution that simulates the physics of phase-change memory cells.
% In cases like this where the distribution is unanalyzable, we need
% techniques for \passert evaluation that can treat arbitrary empirical
% distributions as black boxes.

% Previous work on approximate programming has used probabilistic
% reasoning to prove statistical properties of programs that incorporate
% randomness.  Specifically, 


% Our
% work, when applied to approximate computing, complements these static
% approaches to accuracy analysis in cases when sampling is more
% practical than conservative analyses.



% w
%\xxx[Kathryn]{where should the few sentences below go?}

\paragraph{Other Forms of Randomness} % Other fields have considered the impacts of nondeterminism.
Randomized algorithms solve computational problems
probabilistically that are intractable to solve deterministically.
Analyzing a randomized algorithm amounts to proving that its output satisfies
a predicate with high probability, which resembles the guarantees given by
checking probabilistic assertions, but is beyond our scope.


% A large body of work has also studied the nondeterminism of programs running on
% multiprocessor machines. This paper does not consider that source of
% nondeterminism, although future work could incorporate probabilistic models of
% thread interleavings to analyze the behavior of parallel software.
% \xxx{citations?}

